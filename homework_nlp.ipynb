{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26210d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7e63e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    \"\"\"A function for calculating accuracy scores\"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return (np.sum( pred_flat == labels_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6cd73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess1(dataset_path,csv_path):\n",
    "    fcsv = open(csv_path,'w',encoding='utf-8',newline='')\n",
    "    csv_writer = csv.writer(fcsv)\n",
    "    # 3. 构建列表头\n",
    "    csv_writer.writerow([\"Text\",\"Labels\"])\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        reader=csv.reader(f)\n",
    "        ln = -1\n",
    "        for line in (reader):\n",
    "            ln = ln+1\n",
    "            if ln==0:\n",
    "                continue\n",
    "            text = line[1]\n",
    "            label = line[2]\n",
    "            text_a = text.split(\"__eou__\")\n",
    "            #print(text_a)\n",
    "            talk_num = len(text_a)\n",
    "            bias = len(label)\n",
    "            if talk_num == bias:\n",
    "                for i in range(talk_num):\n",
    "                    csv_writer.writerow([text_a[i],label[i]])\n",
    "                \n",
    "            \n",
    "def data_preprocess2(dataset_path,csv_path):\n",
    "    vcsv = open(csv_path,'w',encoding='utf-8',newline='')\n",
    "    tcsv = open(r\"./test_1.csv\",'w',encoding='utf-8',newline='')\n",
    "    csv_w1 = csv.writer(vcsv)\n",
    "    csv_w2 = csv.writer(tcsv)\n",
    "    csv_w1.writerow([\"Text\",\"Labels\"])\n",
    "    csv_w2.writerow([\"ID\",\"Last Label\"])\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        readpre=csv.reader(f)\n",
    "        ln = 0\n",
    "        for line in (readpre):\n",
    "            if ln==0:\n",
    "                continue\n",
    "            pre_text = line[1]\n",
    "            pre_label = line[2]\n",
    "            break\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        reader=csv.reader(f)\n",
    "        ln = -1\n",
    "        for line in (reader):\n",
    "            ln = ln+1\n",
    "            if ln==0:\n",
    "                continue\n",
    "            text = line[1]\n",
    "            label = line[2]\n",
    "            text_a = text.split(\"__eou__\")\n",
    "            talk_num = len(text_a)\n",
    "            bias = len(label)\n",
    "            if bias+1 == talk_num:\n",
    "                for i in range(talk_num-1):\n",
    "                    csv_w1.writerow([text_a[i],label[i]])\n",
    "                csv_w2.writerow([text_a[talk_num-1],1])\n",
    "                pre_text = text\n",
    "                pre_label = label\n",
    "            else:\n",
    "                text_a = pre_text.split(\"__eou__\")\n",
    "                talk_num = len(text_a)\n",
    "                for i in range(talk_num-1):\n",
    "                    csv_w1.writerow([text_a[i],pre_label[i]])\n",
    "                csv_w2.writerow([text_a[talk_num-1],1])\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da5395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig, AdamW, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer,  RobertaForSequenceClassification\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import csv\n",
    "import random\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "bert_path = \"./bert-base-chinese\"\n",
    "\n",
    "def set_seed(seed=7):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def read_data(path, tokenizers, max_len):\n",
    "    input_ids = []\n",
    "    input_types = []\n",
    "    input_masks = []\n",
    "    input_labels = []\n",
    "    ln = -1\n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)       \n",
    "        for line in (reader):\n",
    "            ln += 1\n",
    "            if ln == 0:\n",
    "                continue\n",
    "            label = int(line[1])\n",
    "            text = '[CLS]' + line[0]\n",
    "            text = tokenizers.tokenize(text)\n",
    "            ids = tokenizers.convert_tokens_to_ids(text)\n",
    "            types = [0] * len(ids)\n",
    "            masks = [1] * len(ids)\n",
    "            if len(ids) <max_len:\n",
    "                types = types + [1]*(max_len - len(ids))\n",
    "                masks = masks + [0] * (max_len - len(ids))\n",
    "                ids = ids + [0] * (max_len - len(ids))\n",
    "            else:\n",
    "                types = types[:max_len]\n",
    "                masks = masks[:max_len]\n",
    "                ids = ids[:max_len]\n",
    "            wrong +=1\n",
    "            assert len(ids) == len(masks) == len(types) == max_len\n",
    "            right +=1\n",
    "            input_masks.append(masks)\n",
    "            input_types.append(types)\n",
    "            input_ids.append(ids)\n",
    "            input_labels.append(label)\n",
    "    print(right,wrong)\n",
    "    input_ids = torch.tensor([i for i in input_ids], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([i for i in input_masks], dtype=torch.long)\n",
    "    token_type_ids = torch.tensor([i for i in input_types], dtype=torch.long)\n",
    "    label_ids = torch.tensor([i for i in input_labels], dtype=torch.long)\n",
    "    data = TensorDataset(input_ids, attention_mask, token_type_ids, label_ids)\n",
    "    return data, len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "078d8a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1(path, epoch, max_len):\n",
    "    set_seed(32)\n",
    "    tokenizers = BertTokenizer.from_pretrained(bert_path, cache_dir=None)\n",
    "    #tokenizers = RobertaTokenizer.from_pretrained(bert_path)\n",
    "    train_data, _ = read_data(path+'train.csv', tokenizers, max_len)\n",
    "    dev_data, dev_len = read_data(path + 'valid.csv', tokenizers, max_len)\n",
    "    test_data, test_len = read_data(path + 'test_1.csv', tokenizers, max_len)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    dev_dataloader = DataLoader(dev_data, batch_size=batch_size, shuffle=True)\n",
    "    #test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    config = BertConfig.from_pretrained(bert_path, num_labels=7)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(bert_path,config = config)\n",
    "    #model = RobertaForSequenceClassification.from_pretrained(bert_path, return_dict=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * epoch\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    best_res = 0.0\n",
    "    best_model = model\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        total_loss, total_val_loss = 0, 0\n",
    "        total_eval_accuracy = 0\n",
    "        for j, batch in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            token_type_ids = batch[2].to(device)\n",
    "            label_ids = batch[3].to(device)\n",
    "            outputs = model(input_ids = input_ids,\n",
    "                           attention_mask = attention_mask,\n",
    "                           token_type_ids = token_type_ids ,\n",
    "                           labels = label_ids)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            #if j% 100 == 0:\n",
    "                #print(i, j, '\\t', loss)\n",
    "        #confusion = torch.zeros(2, 2)\n",
    "        model.eval()\n",
    "        for k, batch in enumerate(dev_dataloader):\n",
    "            with torch.no_grad():\n",
    "                input_ids = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                token_type_ids = batch[2].to(device)\n",
    "                label_ids = batch[3].to(device)\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                                     attention_mask=attention_mask,\n",
    "                                     token_type_ids=token_type_ids,\n",
    "                                     labels=label_ids)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                total_val_loss += loss.item()\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = label_ids .to('cpu').numpy()\n",
    "                pred = np.argmax(logits, axis=1).flatten()\n",
    "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        avg_val_loss = total_val_loss / len(dev_dataloader)\n",
    "        avg_val_accuracy = total_eval_accuracy / dev_len\n",
    "        #f1 = flat_f1(confusion)\n",
    "        if avg_val_accuracy > best_res:\n",
    "            best_model = model\n",
    "    best_model.eval()\n",
    "    add_data = []\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            token_type_ids = batch[2].to(device)\n",
    "            label_ids = batch[3].to(device)\n",
    "            outputs = best_model(input_ids=input_ids,\n",
    "                                 attention_mask=attention_mask,\n",
    "                                 token_type_ids=token_type_ids,\n",
    "                                 labels=label_ids)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_test_loss += loss.item()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            total_test_accuracy += flat_accuracy(logits, label_ids)\n",
    "            pred = np.argmax(logits, axis=1).flatten()            \n",
    "            for m in range(len(pred)):\n",
    "                add_data.append(pred[m])\n",
    "    former = pd.read_csv(r'./text2.csv') \n",
    "    former['Last Label'] = pred\n",
    "    data.to_csv(r'./test_2.csv',mode='a',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2019c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36819 36819\n",
      "3340 3340\n",
      "1000 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "path = \"./\"\n",
    "max_len = 128\n",
    "epoch = 5\n",
    "train_org = \"./train_data.csv\"\n",
    "train_up = \"./train.csv\"\n",
    "valid_org = \"./test_data_new.csv\"def flat_accuracy(preds, labels):\n",
    "    \"\"\"A function for calculating accuracy scores\"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return (np.sum( pred_flat == labels_flat))\n",
    "valid_up = \"./valid.csv\"\n",
    "#data_preprocess1(train_org,train_up)\n",
    "#data_preprocess2(valid_org,valid_up)\n",
    "step1(path,max_len,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c57854a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Text', 'Labels'], dtype='object')\n",
      "                                                 Text Labels\n",
      "0                                  我就奇怪了  为啥你能拍得这么美呢       3\n",
      "1                                   因为我做什么都认真，都诚心诚意！       2\n",
      "2                     好你诚心诚意！我谦虚低调！咱都是优秀品格的人再赞一个  干杯       2\n",
      "3                                       嗯嗯，咱俩都是最可爱的人！      2\n",
      "4      是这是人家自己的事就算我能见到她也不会说你们分手吧什么的可是我真心不喜欢冯绍峰这个理由够吗       5\n",
      "...                                               ...    ...\n",
      "73634                                         那你想想就好了      6\n",
      "73635                                      脸好小啊啊啊啊羡慕       2\n",
      "73636                    很难，我都胖成麻瓜那样了，因为脸不长肉，谁都给我硬塞饭       3\n",
      "73637                            你这太好看了，我脸也胖身上也胖呜呜呜呜       3\n",
      "73638            匀称点好！我腰以下疯长肉，腰以皮包骨。难受，我有两张都是老u的衣服哈哈哈      1\n",
      "\n",
      "[73639 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(r'./train.csv')   #打开一个csv，得到data对象\n",
    "print(data.columns)#获取列索引值\n",
    "data1 = data['Labels']#获取name列的数据\n",
    "data1[0] = 3\n",
    "#data1[1] = '3'\n",
    "data['Labels'] = data1 #将数据插入新列new\n",
    "data.to_csv(r\"./train.csv\",mode = 'a',index =False)\n",
    "#保存到csv,  mode=a，以追加模式写入,header表示列名，默认为true,index表示行名，默认为true，再次写入不需要行名\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f1981898",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"./train.csv\",mode = 'a',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bea088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_writer.writerow([str, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcabbb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'a']\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "a = \"aabaaaba\"\n",
    "print(a.split('b'))\n",
    "print(a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0d7fd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x = i\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31a089c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(a.split('b')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d3340b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
